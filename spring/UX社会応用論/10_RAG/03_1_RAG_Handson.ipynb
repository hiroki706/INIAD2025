{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJQ9LY48q5a5"
      },
      "source": [
        "# 1.LLMを導入してみよう"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InNPx_oGGkQt"
      },
      "source": [
        "## 1.1.必要なライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9PcGueRndzJ"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-openai gradio python-dotenv langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMMeugUpGpTl"
      },
      "source": [
        "## 1.2.ChatGPTのAPIキーの登録"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7hj9IPQnor-"
      },
      "outputs": [],
      "source": [
        "# APIキーの登録\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'yyHDWJTRyxJBtgsEIE3Y7_BLZOHs5XDCVr0LfZQrzoOJ8cjsFqoy4-lYb5orGEwrH3OrwDEDq2wDc3TEJauWCHA'\n",
        "\n",
        "# openai_api_base=\"https://api.openai.com/v1/\"\n",
        "openai_api_base=\"https://api.openai.iniad.org/api/v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OREs7gMFGtS9"
      },
      "source": [
        "## 1.3.Chatbotの起動"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1hkmegznYwl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.schema import HumanMessage, AIMessage\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 固定パラメータ設定\n",
        "MODEL_NAME = \"gpt-4o-mini\"\n",
        "TEMPERATURE = 0.7\n",
        "\n",
        "class OpenAIChatbot:\n",
        "    def __init__(self):\n",
        "        # APIキーを環境変数から取得\n",
        "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"OPENAI_API_KEYが環境変数に設定されていません。\")\n",
        "\n",
        "        # グローバル変数として宣言された openai_api_base を使用\n",
        "        global openai_api_base\n",
        "        print(f\"✅ OpenAI Base URL: {openai_api_base}\")\n",
        "\n",
        "        # モデル初期化\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=MODEL_NAME,\n",
        "            temperature=TEMPERATURE,\n",
        "            base_url=openai_api_base,\n",
        "            openai_api_key=api_key,\n",
        "            streaming=False\n",
        "        )\n",
        "\n",
        "        self.memory = ConversationBufferWindowMemory(\n",
        "            k=10,\n",
        "            return_messages=True\n",
        "        )\n",
        "\n",
        "        self.system_prompt = \"\"\"あなたは親切で知識豊富なAIアシスタントです。\n",
        "ユーザーの質問に対して、正確で有用な情報を提供してください。\n",
        "わからないことがあれば、素直に「わからない」と答えてください。\n",
        "日本語で自然な会話を心がけてください。\"\"\"\n",
        "\n",
        "    def chat(self, user_input):\n",
        "      messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n",
        "      for msg in self.memory.chat_memory.messages:\n",
        "          if isinstance(msg, HumanMessage):\n",
        "              messages.append({\"role\": \"user\", \"content\": msg.content})\n",
        "          elif isinstance(msg, AIMessage):\n",
        "              messages.append({\"role\": \"assistant\", \"content\": msg.content})\n",
        "      messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "      try:\n",
        "          with get_openai_callback() as cb:\n",
        "              response = self.llm.invoke(messages)\n",
        "          self.memory.chat_memory.add_user_message(user_input)\n",
        "          self.memory.chat_memory.add_ai_message(response.content)\n",
        "          return response.content\n",
        "      except Exception as e:\n",
        "          print(f\"❌ Error during chat: {e}\")\n",
        "          return f\"⚠️ エラーが発生しました: {e}\"\n",
        "\n",
        "\n",
        "# インスタンス生成\n",
        "chatbot = OpenAIChatbot()\n",
        "\n",
        "def chat_response(message, history):\n",
        "    return chatbot.chat(message)\n",
        "\n",
        "# Gradio Chat UI のみ\n",
        "def create_ui():\n",
        "    return gr.ChatInterface(\n",
        "        fn=chat_response,\n",
        "        title=\"ChatBot（ChatGPT利用）\",\n",
        "        description=\"OpenAIによる日本語チャットアシスタントです。\",\n",
        "        examples=[\n",
        "            \"INIAD工業株式会社の就業規定について教えてください\",\n",
        "            \"装置Aの停止時の適切な対応方法について教えてください\"\n",
        "        ],\n",
        "        submit_btn=\"送信\",\n",
        "        stop_btn=\"停止\",\n",
        "    )\n",
        "\n",
        "# 実行\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_ui()\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-qnYvpalFEQ"
      },
      "source": [
        "# 2.RAGを導入してみよう\n",
        "このノートブックでは、LangChainとChromaを使ってPDFドキュメントから情報を取り込み、チャンク化し、OpenAIの埋め込みモデルを使ってベクトルストアを構築し、簡単な質問応答（RAG）を実践します。\n",
        "\n",
        "*動作環境: Google Colabを想定しています。*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmrVP39ilK0e"
      },
      "source": [
        "## 2.1. ライブラリのインストール\n",
        "必要なパッケージをインストールします。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OifkbEWkFB2"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install langchain chromadb langchain-chroma langchain-openai pypdf tiktoken langchain-community requests gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q50rsFBClOpK"
      },
      "source": [
        "## 2.2. 必要なモジュールのインポート\n",
        "必要なモジュールを読み込みます。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WFyPQVx9kMCu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "import gradio as gr\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyM3LORtlsC7"
      },
      "source": [
        "## 2.3. PDFドキュメントのダウンロード\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFDJ_oavlb1x"
      },
      "outputs": [],
      "source": [
        "!wget -O rag_sample.pdf \"https://www.dl.dropboxusercontent.com/scl/fi/5q5wgjugmc7v0vc4youde/.pdf?rlkey=3mudbfkqjb7xj86hffmh962v1&st=toe9octw&dl=1\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uky-WiXlyiV"
      },
      "source": [
        "## 2.4. ドキュメントの読み込みとチャンク化\n",
        "PDFを読み込み、長いテキストを扱いやすいサイズに分割します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvV_3gb_lt4N"
      },
      "outputs": [],
      "source": [
        "# PDFローダーでドキュメントを取得\n",
        "loader = PyPDFLoader(\"/content/rag_sample.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# チャンク化の設定（例: 1000文字のチャンク、重複200文字）\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=20\n",
        ")\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"Created {len(chunks)} chunks.\")\n",
        "\n",
        "for chunk in chunks:\n",
        "  print(f\"====================================\")\n",
        "  print(chunk)\n",
        "  print(f\"====================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQQyuWfxmEK_"
      },
      "source": [
        "## 2.5. チャンクのベクトル化とベクトルの保存\n",
        "OpenAIEmbeddingモデルでテキストチャンクを埋め込み、Chromaに保存します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuXr8TX4l7Vp"
      },
      "outputs": [],
      "source": [
        "# 埋め込みモデルの初期化\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    openai_api_base=openai_api_base,\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")\n",
        "# Chromaベクトルストアを作成（ローカルに永続化）\n",
        "vectorstore = Chroma(\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_db/rag_sample\")\n",
        "\n",
        "# チャンクをベクトルDBに追加\n",
        "vectorstore.add_documents(chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwKhxJaDws8m"
      },
      "source": [
        "## 2.6.ベクトル検索をしてみよう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhpnFq3Bwwz5"
      },
      "outputs": [],
      "source": [
        "# 質問の例\n",
        "query = \"INIAD工業株式会社の概要について教えてください\"\n",
        "\n",
        "# ベクトル検索\n",
        "docs = vectorstore.similarity_search(query, k=1)\n",
        "\n",
        "# 結果を表示\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    print(f\"[{i}] {doc.metadata.get('source', '')}\")\n",
        "    print(doc.page_content[:300], \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8GbG0WOmMSu"
      },
      "source": [
        "## 2.7. RAGの作成\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JaEqlPHmG8X"
      },
      "outputs": [],
      "source": [
        "# 生成モデルの初期化\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.0,\n",
        "    openai_api_base=openai_api_base,\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")\n",
        "\n",
        "# QAチェーンの設定\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQonkA4gmnq0"
      },
      "source": [
        "## 2.8. RAGの実行\n",
        "実際にユーザーが質問を入力して回答を得ます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5feTmQbhmTWH"
      },
      "outputs": [],
      "source": [
        "print(\"============================= RAGの回答 =============================\")\n",
        "query = \"INIAD工業株式会社の概要について教えてください\"\n",
        "rag_answer = qa.invoke(query)\n",
        "print(rag_answer[\"result\"])\n",
        "\n",
        "print(\"============================= LLM(ChatGPT)の回答 =============================\")\n",
        "llm_answer = llm.invoke(query)\n",
        "print(llm_answer.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CehA-mhMH-iI"
      },
      "source": [
        "## 2.9.ドキュメントをまとめて処理する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLRMDJnlJ2iH"
      },
      "source": [
        "### 2.9.1. ドキュメントのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q2Sre2-IzCO"
      },
      "outputs": [],
      "source": [
        "# === 設定 ===\n",
        "dropbox_url = \"https://www.dropbox.com/scl/fo/53hp5xxh8jo08t8ckhvyf/APEhI4MajStJbSYZClqvK0k?rlkey=4s5bwt71v39tfp4v075f4xi1l&st=n8dnqbjz&dl=1\"  # 直リンク化\n",
        "zip_path = \"rag_handson.zip\"\n",
        "extract_dir = \"rag_handson\"\n",
        "\n",
        "# === 1. DropboxからZIPをダウンロード ===\n",
        "print(\"Downloading ZIP from Dropbox...\")\n",
        "with requests.get(dropbox_url, stream=True) as r:\n",
        "    r.raise_for_status()\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "# === 2. ZIPファイルを解凍 ===\n",
        "print(\"Extracting ZIP...\")\n",
        "with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(\"Extraction complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q42qcoi_J9FH"
      },
      "source": [
        "### 2.9.2. ダウンロードしたファイルを一括でベクトルDBに保存する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbNvku14KHxo"
      },
      "outputs": [],
      "source": [
        "# === 1. PDF読み込み & チャンク化 ===\n",
        "docs = []\n",
        "for path in Path(extract_dir).rglob(\"*.pdf\"):\n",
        "    try:\n",
        "        loader = PyPDFLoader(str(path))\n",
        "        docs.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"❌ {path.name} の読み込みに失敗しました: {e}\")\n",
        "\n",
        "\n",
        "print(f\" {len(docs)} ページのPDFを読み込みました。\")\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\" {len(chunks)} 個のチャンクに分割されました。\")\n",
        "\n",
        "# === 2. 埋め込みとベクトルストアの作成 ===\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    openai_api_base=openai_api_base,\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")\n",
        "vectorstore = Chroma(\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_db/rag_handson\"\n",
        ")\n",
        "vectorstore.add_documents(chunks)\n",
        "print(\"ベクトルDBに保存が完了しました。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-U9nA8oIFqG"
      },
      "source": [
        "## 2.10. Chatbot（RAG）の起動"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CoQNa-dBn6O2"
      },
      "outputs": [],
      "source": [
        "# --- 埋め込みモデルとベクターストア初期化 ---\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    openai_api_base=openai_api_base,\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY']\n",
        ")\n",
        "vectorstore = Chroma(\n",
        "    persist_directory=\"./chroma_db/rag_handson\",\n",
        "    embedding_function=embeddings\n",
        ")\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})  # 上位3件を返す\n",
        "\n",
        "\n",
        "# --- LLM 初期化 ---\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.0,\n",
        "    base_url=openai_api_base,\n",
        "    openai_api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "\n",
        "# --- カスタムプロンプト ---\n",
        "template = \"\"\"以下はコンテキストと質問です。\n",
        "コンテキストのみを使って質問に答えてください。\n",
        "わからなければ「わかりません」と答えてください。\n",
        "\n",
        "コンテキスト:\n",
        "{context}\n",
        "\n",
        "質問:\n",
        "{question}\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "# --- RetrievalQA チェーンの作成（ソース取得付き） ---\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=retriever,\n",
        "    chain_type=\"stuff\",\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "\n",
        "# --- 回答関数（ドキュメントも含める） ---\n",
        "def chat_response(message, history):\n",
        "    try:\n",
        "        result = qa_chain.invoke({\"query\": message})\n",
        "        answer = result.get(\"result\", \"申し訳ありませんが、わかりません。\")\n",
        "        sources = result.get(\"source_documents\", [])\n",
        "\n",
        "        if sources:\n",
        "            formatted_sources = []\n",
        "            for i, doc in enumerate(sources):\n",
        "                title = doc.metadata.get(\"source\", f\"ドキュメント{i+1}\")\n",
        "                snippet = doc.page_content.strip().replace(\"\\n\", \" \").replace(\"　\", \" \")\n",
        "                snippet = snippet[:300] + (\"…\" if len(snippet) > 300 else \"\")\n",
        "                formatted_sources.append(f\"\"\"\n",
        "【{i+1}. {title}】\n",
        "{snippet}\n",
        "\"\"\")\n",
        "\n",
        "            sources_text = \"\\n---\\n\".join(formatted_sources)\n",
        "            full_response = f\"{answer}\\n\\n📄 **参照ドキュメント**\\n{sources_text}\"\n",
        "        else:\n",
        "            full_response = answer\n",
        "\n",
        "        return full_response\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return f\"エラーが発生しました: {str(e)}\"\n",
        "\n",
        "\n",
        "# --- Gradio UI ---\n",
        "def create_ui():\n",
        "    return gr.ChatInterface(\n",
        "        fn=chat_response,\n",
        "        title=\"ChatBot（RAG + ソース表示付き）\",\n",
        "        description=\"PDFなどから構築されたベクターストアを用いた質問応答。参照元も表示します。\",\n",
        "        examples=[\n",
        "            \"INIAD工業株式会社の就業規定について教えてください\",\n",
        "            \"装置Aの停止時の適切な対応方法について教えてください\"\n",
        "        ],\n",
        "        submit_btn=\"送信\",\n",
        "        stop_btn=\"停止\",\n",
        "        type=\"messages\"\n",
        "    )\n",
        "\n",
        "# --- 実行 ---\n",
        "if __name__ == \"__main__\":\n",
        "    demo = create_ui()\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCKKNrO6phzc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HJQ9LY48q5a5",
        "InNPx_oGGkQt",
        "SMMeugUpGpTl",
        "OREs7gMFGtS9",
        "FmrVP39ilK0e",
        "q50rsFBClOpK",
        "uyM3LORtlsC7",
        "6uky-WiXlyiV",
        "YQQyuWfxmEK_",
        "DwKhxJaDws8m",
        "e8GbG0WOmMSu",
        "CehA-mhMH-iI",
        "HLRMDJnlJ2iH",
        "q42qcoi_J9FH",
        "t-U9nA8oIFqG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
